{{- if or (eq $.Values.cluster_type "k3s-k3d") (and (eq $.Values.cluster_type "eks") (eq (toString $.Values.couchdb_data.preExistingDataAvailable) "true")) }}
{{- if eq (toString .Values.couchdb.clusteredCouch_enabled) "false" }}
# Single node persistent volume
apiVersion: v1
kind: PersistentVolume
metadata:
  name: couchdb-pv-{{ .Values.namespace }}
spec:
  capacity:
    storage: {{ .Values.ebs.preExistingEBSVolumeSize | default "100Gi" }}  # Fallback for backward compatibility
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: {{ index .Values.ebs "preExistingEBSVolumeID-1" }}
    fsType: ext4
    volumeAttributes:
      partition: "{{ .Values.couchdb_data.partition | default "0" }}"  # Fallback for backward compatibility
{{- else }}
# Multi-node persistent volumes
# If we're using clustered mode and the cluster is k3s-k3d or eks and pre-existing data is available, then we need to create PersistentVolume for each CouchDB node.
# If the cluster is eks and there is no pre-existing data, this is not applicable as those are created automatically when making the claims.
{{- range $i, $e := until (int $.Values.clusteredCouch.noOfCouchDBNodes) }}
{{ $nodeNumber := add $i 1 }}
apiVersion: v1
kind: PersistentVolume
metadata:
  name: couchdb-pv-{{ $.Values.namespace }}-{{ $nodeNumber }}
spec:
  capacity:
    storage: {{ $.Values.couchdb.couchdb_node_storage_size | default "100Mi"}}  # Fallback for backward compatibility
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  {{- if eq $.Values.cluster_type "eks" }}
  csi:
    driver: ebs.csi.aws.com
    volumeHandle: {{ index $.Values.ebs (printf "preExistingEBSVolumeID-%d" $nodeNumber) }}
    fsType: ext4
    volumeAttributes:
      partition: "{{ $.Values.couchdb_data.partition | default "0" }}"  # Fallback for backward compatibility
  {{- else if eq $.Values.cluster_type "k3s-k3d" }}
  storageClassName: {{ if and ($.Values.k3s_use_vSphere_storage_class) (eq (toString $.Values.k3s_use_vSphere_storage_class) "true") }}vsphere-storage-class{{ else }}local-storage{{ end }}
  {{- if and ($.Values.k3s_use_vSphere_storage_class) (eq (toString $.Values.k3s_use_vSphere_storage_class) "true") }}
  vsphereVolume:
    volumePath: "[{{ $.Values.vSphere.datastoreName }}] {{ $.Values.vSphere.diskPath }}/{{ $.Values.namespace }}-couchdb{{ $nodeNumber }}"
    fsType: ext4
  {{- else }}
  local:
    path: {{- if eq (toString $.Values.couchdb_data.preExistingDataAvailable) "true" }}
            {{- if hasKey $.Values.local_storage (printf "preExistingDiskPath-%d" $nodeNumber) }}
              {{ index $.Values.local_storage (printf "preExistingDiskPath-%d" $nodeNumber) }}
            {{- else }}
              /var/lib/{{ $.Values.namespace }}-couchdb{{ $nodeNumber }}
            {{- end }}
          {{- else }}
            /var/lib/{{ $.Values.namespace }}-couchdb{{ $nodeNumber }}
          {{- end }}
  {{- end }}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - {{ index $.Values.nodes (printf "node-%d" $nodeNumber) }}
  {{- end }}
--- #Don't remove the separator. We need this to separate yamls generated by the range command.
{{- end }}
{{- end }}
{{- end }}
