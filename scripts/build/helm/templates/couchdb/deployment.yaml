{{- include "cht-chart-4x.validations" . -}}

{{- if not .Values.couchdb.clusteredCouchEnabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    cht.service: couchdb
  name: cht-couchdb
spec:
  replicas: 1
  selector:
    matchLabels:
      cht.service: couchdb
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        cht.service: couchdb
    spec:
      terminationGracePeriodSeconds: 0
      securityContext:
        runAsUser: 0  # Run as root
        fsGroup: 0    # Set fsGroup to root
      {{- if .Values.toleration }}
      tolerations:
        - key: {{ .Values.toleration.key }}
          operator: {{ .Values.toleration.operator }}
          value: {{ .Values.toleration.value | quote }}
          effect: {{ .Values.toleration.effect }}
      {{- end }}
      {{- if and (hasKey .Values "nodes") (kindIs "map" .Values.nodes) }}
        {{- if hasKey .Values.nodes "single_node_deploy" }}
      nodeSelector:
        kubernetes.io/hostname: {{ .Values.nodes.single_node_deploy }}
        {{- end }}
      {{- end }}
      containers:
        - name: cht-couchdb
          image: "{{ .Values.upstream_servers.docker_registry }}/cht-couchdb:{{ .Values.cht_image_tag }}"
          {{ if eq .Values.cache_images false}}imagePullPolicy: Always{{ end }}
          ports:
            - containerPort: 5984
          env:
{{- include "couchdb.basicEnv" . | nindent 12 }}
            - name: SVC_NAME
              value: couchdb.{{ .Values.namespace }}.svc.cluster.local
          resources: {}
          {{- if eq (toString .Values.cluster_type) "k3s-k3d" }}
          volumeMounts:
            - mountPath: /opt/couchdb/data
              name: couchdb-claim0
              {{- if .Values.couchdb_data.dataPathOnDiskForCouchDB }}
              subPath: {{ .Values.couchdb_data.dataPathOnDiskForCouchDB }}
              {{- end }}
            - mountPath: /opt/couchdb/etc/local.d
              name: couchdb-claim0
              subPath: local.d
          {{- else }}
          volumeMounts:
            - mountPath: /opt/couchdb/data
              name: couchdb-claim0
              {{- if .Values.couchdb_data.dataPathOnDiskForCouchDB }}
              subPath: {{ .Values.couchdb_data.dataPathOnDiskForCouchDB }}
              {{- end }}
            - mountPath: /opt/couchdb/etc/local.d
              name: couchdb-claim0
              subPath: local.d
          {{- end }}
        - name: cht-couchdb-nouveau
          image: "{{ .Values.upstream_servers.docker_registry }}/cht-couchdb-nouveau:{{ .Values.cht_image_tag }}"
          {{ if eq .Values.cache_images false}}imagePullPolicy: Always{{ end }}
          ports:
            - containerPort: 5987
          resources: {}
          volumeMounts:
            - mountPath: /data/nouveau
              name: couchdb-claim0
              subPath: data
      restartPolicy: Always
      volumes:
        - name: couchdb-claim0
          persistentVolumeClaim:
            claimName: couchdb-claim0
status: {}
{{- end }}

{{- if .Values.couchdb.clusteredCouchEnabled }}
{{- range $i, $e := until (int .Values.clusteredCouch.noOfCouchDBNodes) }}
{{ $nodeNumber := add $i 1 }}
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    cht.service: couchdb-{{ $nodeNumber }}
  name: cht-couchdb-{{ $nodeNumber }}
spec:
  replicas: 1
  selector:
    matchLabels:
      cht.service: couchdb-{{ $nodeNumber }}
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        cht.service: couchdb-{{ $nodeNumber }}
    spec:
      terminationGracePeriodSeconds: 0
      securityContext:
        runAsUser: 0  # Run as root
        fsGroup: 0    # Set fsGroup to root
      {{- if $.Values.toleration }}
      tolerations:
        - key: {{ $.Values.toleration.key }}
          operator: {{ $.Values.toleration.operator }}
          value: {{ $.Values.toleration.value | quote }}
          effect: {{ $.Values.toleration.effect }}
      {{- end }}
      {{- if and (hasKey $.Values "nodes") (kindIs "map" $.Values.nodes) }}
        {{- with index $.Values.nodes (printf "node-%d" $nodeNumber) }}
          {{- if . }}
      nodeSelector:
        kubernetes.io/hostname: {{ . }}
          {{- end }}
        {{- end }}
      {{- end }}
      containers:
      - name: cht-couchdb-{{ $nodeNumber }}
        image: "{{ $.Values.upstream_servers.docker_registry }}/cht-couchdb:{{ $.Values.cht_image_tag }}"
        {{ if eq $.Values.cache_images false}}imagePullPolicy: Always{{ end }}
        ports:
          - containerPort: 5984
        env:
        {{- if ne $nodeNumber 1 }}
          - name: COUCHDB_SYNC_ADMINS_NODE
            valueFrom:
              configMapKeyRef:
                name: couchdb-servers-configmap
                key: COUCHDB_SYNC_ADMINS_NODE
        {{- end }}
{{- include "couchdb.basicEnv" $ | nindent 10 }}
          - name: SVC_NAME
            value: couchdb-{{ $nodeNumber }}.{{ $.Values.namespace }}.svc.cluster.local
          - name: NODE_COUNT
            value: {{ $.Values.clusteredCouch.noOfCouchDBNodes | quote }}
        {{- if eq $nodeNumber 1 }}
          - name: CLUSTER_PEER_IPS
            valueFrom:
              configMapKeyRef:
                name: couchdb-servers-configmap
                key: CLUSTER_PEER_IPS
        {{- end }}
        resources: {}
        volumeMounts:
        - mountPath: /opt/couchdb/data
          name: couchdb-{{ $nodeNumber }}-claim0
          {{- if $.Values.couchdb_data.dataPathOnDiskForCouchDB }}
          subPath: {{ if contains "%d" $.Values.couchdb_data.dataPathOnDiskForCouchDB }}
                    {{- printf $.Values.couchdb_data.dataPathOnDiskForCouchDB $nodeNumber }}
                    {{- else }}
                    {{- $.Values.couchdb_data.dataPathOnDiskForCouchDB }}
                    {{- end }}
          {{- end }}
        - mountPath: /opt/couchdb/etc/local.d
          name: couchdb-{{ $nodeNumber }}-claim0
          subPath: local.d
      {{- if eq $nodeNumber 1 }}
      - name: cht-couchdb-nouveau
        image: "{{ $.Values.upstream_servers.docker_registry }}/cht-couchdb-nouveau:{{ $.Values.cht_image_tag }}"
        {{ if eq $.Values.cache_images false}}imagePullPolicy: Always{{ end }}
        ports:
          - containerPort: 5987
        resources: {}
        volumeMounts:
        - mountPath: /data/nouveau
          name: couchdb-{{ $nodeNumber }}-claim0
          subPath: data
      {{- end }}
      restartPolicy: Always
      volumes:
      - name: couchdb-{{ $nodeNumber }}-claim0
        persistentVolumeClaim:
          claimName: couchdb-{{ $nodeNumber }}-claim0
status: {}
--- #Don't remove the separator. We need this to separate yamls generated by the range command.
{{- end }}
{{- end }}
