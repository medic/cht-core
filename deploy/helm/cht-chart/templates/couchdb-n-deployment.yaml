{{- $root := . -}}
{{- if .Values.couchdb.clusteredCouch_enabled }}
{{- range $i, $e := until (int .Values.clusteredCouch.noOfCouchDBNodes) }}
{{ $nodeNumber := add $i 1 }}
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    cht.service: couchdb-{{ $nodeNumber }}
  name: cht-couchdb-{{ $nodeNumber }}
spec:
  replicas: 1
  selector:
    matchLabels:
      cht.service: couchdb-{{ $nodeNumber }}
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        cht.service: couchdb-{{ $nodeNumber }}
    spec:
      tolerations:
      - key: {{ $root.Values.toleration.key }}
        operator: {{ $root.Values.toleration.operator }}
        value: {{ $root.Values.toleration.value | quote }}
        effect: {{ $root.Values.toleration.effect }}
      containers:
      - name: cht-couchdb-{{ $nodeNumber }}
        image: public.ecr.aws/medic/cht-couchdb:{{ $root.Values.cht_image_tag }}
        ports:
        - containerPort: 5984
        env:
        #if it's not the first node, we need to set the COUCHDB_SYNC_ADMINS_NODE env var
        {{- if ne $nodeNumber 1 }}
        - name: COUCHDB_SYNC_ADMINS_NODE
          valueFrom:
            configMapKeyRef:
              name: couchdb-servers-configmap
              key: COUCHDB_SYNC_ADMINS_NODE
        {{- end }}        
        - name: COUCHDB_LOG_LEVEL
          value: error
        - name: COUCHDB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: cht-couchdb-credentials
              key: COUCHDB_PASSWORD
        - name: COUCHDB_SECRET
          valueFrom:
            secretKeyRef:
              name: cht-couchdb-credentials
              key: COUCHDB_SECRET
        - name: COUCHDB_USER
          valueFrom:
            secretKeyRef:
              name: cht-couchdb-credentials
              key: COUCHDB_USER
        - name: COUCHDB_UUID
          valueFrom:
            secretKeyRef:
              name: cht-couchdb-credentials
              key: COUCHDB_UUID
        - name: SVC_NAME
          value: couchdb-{{ $nodeNumber }}.{{ $root.Values.namespace }}.svc.cluster.local
        - name: NODE_COUNT
          value: {{ $root.Values.clusteredCouch.noOfCouchDBNodes | quote }}
        #if it's the first node, we need to set the CLUSTER_PEER_IPS env var
        {{- if eq $nodeNumber 1 }}
        - name: CLUSTER_PEER_IPS
          valueFrom:
            configMapKeyRef:
              name: couchdb-servers-configmap
              key: CLUSTER_PEER_IPS
        {{- end }}
        resources: {}
        {{- if eq $root.Values.environment "local" }}
        volumeMounts:
        - mountPath: /opt/couchdb/data
          name: local-volume
          subPath: data
        - mountPath: /opt/couchdb/etc/local.d
          name: local-volume
          subPath: local.d
        {{- else }}
        volumeMounts:
        - mountPath: /opt/couchdb/data
          name: couchdb-{{ $nodeNumber }}-claim0
          subPath: data
        - mountPath: /opt/couchdb/etc/local.d
          name: couchdb-{{ $nodeNumber }}-claim0
          subPath: local.d
        {{- end }}
      restartPolicy: Always
      {{- if eq $root.Values.environment "local" }}
      volumes:
        - name: local-volume
          hostPath:
            path: {{ $root.Values.local.diskPath }}
      {{- else }}
      volumes:
        - name: couchdb-{{ $nodeNumber }}-claim0
          persistentVolumeClaim:
            claimName: couchdb-{{ $nodeNumber }}-claim0
      {{- end }}
status: {}
--- #Don't remove the separator. We need this to separate yamls generated by the range command.
{{- end }}
{{- end }}
